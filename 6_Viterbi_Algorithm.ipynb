{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Viterbi Algorithm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNAJOb2O0QQOpVPAJQRP55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thecrazyphysicist369/NLP-Lab-Assignment/blob/main/6_Viterbi_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4egBKNv8OOm"
      },
      "source": [
        "##6. Write a python program to implement the Viterbi Algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNLvXZxyk1zC",
        "outputId": "3b49e8c5-1148-40b8-a74e-e6eb24b5bee0"
      },
      "source": [
        "! git clone https://github.com/thecrazyphysicist369/NLP-Lab-Assignment"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP-Lab-Assignment'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 82 (delta 34), reused 0 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQEjk-eUjBva"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk-VLTryjycl"
      },
      "source": [
        "def forward(V, a, b, initial_distribution):\n",
        "    alpha = np.zeros((V.shape[0], a.shape[0]))\n",
        "    alpha[0, :] = initial_distribution * b[:, V[0]]\n",
        " \n",
        "    for t in range(1, V.shape[0]):\n",
        "        for j in range(a.shape[0]):\n",
        "            # Matrix Computation Steps\n",
        "            #                  ((1x2) . (1x2))      *     (1)\n",
        "            #                        (1)            *     (1)\n",
        "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
        " \n",
        "    return alpha\n",
        " "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsQUvLPNj0HK"
      },
      "source": [
        "def backward(V, a, b):\n",
        "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
        " \n",
        "    # setting beta(T) = 1\n",
        "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
        " \n",
        "    # Loop in backward way from T-1 to\n",
        "    # Due to python indexing the actual loop will be T-2 to 0\n",
        "    for t in range(V.shape[0] - 2, -1, -1):\n",
        "        for j in range(a.shape[0]):\n",
        "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
        " \n",
        "    return beta\n",
        " "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxz29lTJj15y"
      },
      "source": [
        "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
        "    M = a.shape[0]\n",
        "    T = len(V)\n",
        " \n",
        "    for n in range(n_iter):\n",
        "        alpha = forward(V, a, b, initial_distribution)\n",
        "        beta = backward(V, a, b)\n",
        " \n",
        "        xi = np.zeros((M, M, T - 1))\n",
        "        for t in range(T - 1):\n",
        "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
        "            for i in range(M):\n",
        "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
        "                xi[i, :, t] = numerator / denominator\n",
        " \n",
        "        gamma = np.sum(xi, axis=1)\n",
        "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
        " \n",
        "        # Add additional T'th element in gamma\n",
        "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
        " \n",
        "        K = b.shape[1]\n",
        "        denominator = np.sum(gamma, axis=1)\n",
        "        for l in range(K):\n",
        "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
        " \n",
        "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
        " \n",
        "    return (a, b)\n",
        " "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MIdP9pnj32C"
      },
      "source": [
        "def viterbi(V, a, b, initial_distribution):\n",
        "    T = V.shape[0]\n",
        "    M = a.shape[0]\n",
        " \n",
        "    omega = np.zeros((T, M))\n",
        "    omega[0, :] = np.log(initial_distribution * b[:, V[0]])\n",
        " \n",
        "    prev = np.zeros((T - 1, M))\n",
        " \n",
        "    for t in range(1, T):\n",
        "        for j in range(M):\n",
        "            # Same as Forward Probability\n",
        "            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, V[t]])\n",
        " \n",
        "            # This is our most probable state given previous state at time t (1)\n",
        "            prev[t - 1, j] = np.argmax(probability)\n",
        " \n",
        "            # This is the probability of the most probable state (2)\n",
        "            omega[t, j] = np.max(probability)\n",
        " \n",
        "    # Path Array\n",
        "    S = np.zeros(T)\n",
        " \n",
        "    # Find the most probable last hidden state\n",
        "    last_state = np.argmax(omega[T - 1, :])\n",
        " \n",
        "    S[0] = last_state\n",
        " \n",
        "    backtrack_index = 1\n",
        "    for i in range(T - 2, -1, -1):\n",
        "        S[backtrack_index] = prev[i, int(last_state)]\n",
        "        last_state = prev[i, int(last_state)]\n",
        "        backtrack_index += 1\n",
        " \n",
        "    # Flip the path array since we were backtracking\n",
        "    S = np.flip(S, axis=0)\n",
        " \n",
        "    # Convert numeric values to actual hidden states\n",
        "    result = []\n",
        "    for s in S:\n",
        "        if s == 0:\n",
        "            result.append(\"A\")\n",
        "        else:\n",
        "            result.append(\"B\")\n",
        " \n",
        "    return result\n",
        " "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IePdUTiqj8sa",
        "outputId": "e451510c-83e3-4469-9fec-031c9fe028a5"
      },
      "source": [
        "data = pd.read_csv('/content/NLP-Lab-Assignment/data/data_viterbi.csv')\n",
        " \n",
        "V = data['Visible'].values\n",
        " \n",
        "# Transition Probabilities\n",
        "a = np.ones((2, 2))\n",
        "a = a / np.sum(a, axis=1)\n",
        " \n",
        "# Emission Probabilities\n",
        "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
        "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
        " \n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        " \n",
        "a, b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
        " \n",
        "output = viterbi(V, a, b, initial_distribution)\n",
        "for i in output:\n",
        "  print(i)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "B\n",
            "B\n",
            "B\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oEq_AtblO2k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}